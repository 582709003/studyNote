交流电经过电容时，电压比较低，记为低电平，用0表示；经过电阻时，电压较高，记为高电平，用1表示

一个0或者1在计算机中被称为一个比特位，然后，如果使用一个位来表示计算机中的最小存储单位，那么这个位
存储的范围就太小了，所以我们就用8个比特位为一组来表示计算机中的最小单元，8个位上每个位能存储
0或者1，则byte范围就是00000000-11111111（换成整数就是0-255），这个最小存储单元就是byte；

对于汉字等一些字符，计算机在内存中提供了一个编码表，这个编码表是记录了每个字符和数字的一一对应

一般繁琐的二进制使用十六进制来表示会比较方便规整，所以人们习惯使用十六进制来表示码值

有哪些编码表呢
    ASCLL:一共128个，最高位用0表示，用一个字节的其他七位就可以表示，是美国指定的，这个方法是一个标准，
            其他编码表中的涉及到相同的字符的都是以ASCLL为标准

    ISO8859-1:西欧的语言，是使用一个字节来表示一个字符，其中0-127与ASCLL一样，128-255规定了不同的含义
    128-159表示了一些控制字符，160-255表示了一些西欧字符

    windows-1252:iso8859-1虽然号称是标准，实际使用中最广泛的是windows-1252,这个编码基本与iso8859-1一样，
    iso8859-1已经被windows-1252取代，现实中大家讨论iso8859-1其实实际是windows-1252，只是大家不知道

    GB2312:主要针对的是简体中文字符，包括约7000个汉字，不包括罕见字以及繁体字，这个编码表固定使用两个
    字节来表示，在两个字节中，最高位都是1，如果是0，就认为是ascill字符，第一个字节的范围是10100001-11110111(161-247)，
    第二个字节范围是10100001-11111110(161-254)两数之差相乘算下来的所有组合大概8178

    GBK:向下兼容了GB2312,也就是说，GB2312编码的字符的二进制表示再GBK里面是完全一样的，GBK增加了一万多个汉字，共计约21000
    汉字，包括繁体字，还是固定使用两个字节表示，其中第一个字节范围10000001(十进制129)-11111110(十进制254)，第二个字节范围
    01000000(十进制64)-01111110(十进制126)和10000000(十进制128)-11111110(十进制254)，需要注意的是第二个字节是从64开始的
    (64属于byte正数范围，和ascill编码重合了)，也就是说，第二个字节的最高位可能为0，那它怎么知道他是汉字的一部分还是ascill字符呢？
    其实很简单，因为汉字使用固定的两个字节表示的，在解析二进制流的时候，如果第一个字节的最高位是1，那么就将下一个字节和其组合起来一起解析
    成汉字，而不用考虑他的最高位，解析完后跳到第三个字节继续解析

    GB18030：向下兼容GBK，增加了五万五千多个字符，包括了少数名族，以及中日韩统一字符，用两个字节已经表示不了这些所有字符，
    ，所以GB18030使用了变长编码，有的字符是两个字节，有的是四个字节，在两个字节的编码中，字节范围和GBK一样，在四个字节的编码中,
    第一个字节值10000001(129)到11111110(254)，第二个字节值00110000(48)-00111001(57)，第三个字节值10000001(129)-11111110(254),
    第四个字节值00110000(48)-00111001(57).
    解析二进制时，如何知道是两个字节还是四个字节表示的一个字符呢？很简单，看gbk和gb18030的第二个字节范围，如果是48-57就是四个字节
    表示。所以综合说明gb18030兼容gbk，gb2312，ascill，但是这些编码和iso8859-1是不兼容的；

    big5编码：针对繁体中文的，香港台湾等地广泛使用，也是两个字节表示，big5和gb18030,gbk，gb2312不兼容哦

注意：以上的编码方式都是已经将字符和字节一一对应好了，不是通过计算得出二进制字节，而是通过给字符分配一个固定的字节，
字符和二进制之间是有一个固定的对应表的；而unicode编码只是给字符分配一个唯一的编号，字符和编号之间有一个固定的对应表；
而真正的编码是通过将编号通过某种规则进行编码成字节，编号和二进制之间需要计算而没有一个固定的对应表

    unicode：为世界上所有的字符都分配一个唯一的数字编号，这个编号范围从0x000000到0x10ffff,
            有一百一十多万个编号，这个编号一般写成16进制，在前面加U+，大部分的中文的编号范围在u+4e00到U+9F45,
            例如，"贤"的unicode是U+8D24.unicode就做了这么一件事，就是给所有的字符分配一个唯一的数字编号，他并没与规定
            这个编号怎么对应到二进制表示，这是与上面介绍的其他编码不同的；
            那么编号如何对应到二进制表示呢？有多种方案，主要有utf-32、utf-16、utf-8
            unicode编码大致过程
                首先为字符分配一个unicode编号，然后再对编号进行utf-32、utf-16、utf-8其中一种进行编码存储

        utf-32:固定的将每个码值以四个字节编码，
        utf-16：处于基本面(处于最前的65536的字符位的分区叫做基本面，剩下的分区就是辅助面)的是2个字节表示，
                处于辅助面的是四个字节表示
                注意：unicode编号是指一个字符被分配的一个唯一的数字编号；而说一个字符的unicode编码默认情况说的就是
               将字符的unicode编号经过utf-16编码过的值，网站上用的unicode编码后的值就是这样的
        utf-8：使用变长字节来表示，每个字符使用的字节个数与其unicode编号大小有关，编号小的使用字节就少，大的使用字节个数就多，
                使用的字节个数从1到4个不等

        utf-8和utf-16/utf-32不同的地方是utf-8是兼容ASCLL的，对于大部分中文而言，一个中文字符需要三个字节
        （所以我们一般还是用GBK编码中文，因为GBK是两个字节的中文，省空间），utf-8的优势是网络上传输英文字符只需要一个字节
        可以节省宽带资源，所以大部分网络的应用都使用utf-8


    utf-8和utf-8+BOM的区别
        用来判断文件使用什么编码来解码，utf-16大端还是小端，utf-32大端还是小端，其实是用来判断编码的字节顺序然后选取是大端
        还是小端来解码，utf-8不需要bom来判断编码顺序，但是可以用这个来判断这是utf-8编码的






    JSP2.3 规范关于 pageEncoding 的第一句话是“该属性描述当前 JSP 页面的字符编码”，这里的“描述”用得也非常准确。
    因为 pageEncoding 只是描述当前 JSP 页面的字符编码，
    至于这个 JSP 文件是不是以该编码方式编码的，那可未必。pageEncoding 只是告诉 JSP 引擎把 JSP 文件转换成 Java 文件的
    时候应该以什么编码方式读取 JSP 页面的内容，但是我jsp页面的编码方式可能和pageencoding不一样。
    比如，我们不借助任何IDE工具，在本地手动新建一个JSP文件。默认情况下，这个JSP文件的编码方式是GBK。如果我们指定pageEncoding
    为UTF-8，相当于编码和解码不一致，那么中文乱码在所难免，一致就可以。

    自我理解：
    服务器在返回html页面时，就是是将html页面以流的形式读到浏览器，这就相当于复制一个页面，而复制页面时的编码就是页面保存时
    的编码格式，然后到浏览器时就应该以这个编码格式进行解码；
    jsp页面也是同样的道理，当访问jsp页面时，服务器先将页面加载页面，加载的过程就相当于复制的过程，也就是流的形式，这时候编码的格式就是jsp页面
    保存时的编码，而pageencoding是解码的格式，所以这两者还是要一样才能避免乱码

    所有的请求都是流，先编码在解码;

    所有的乱码都可以以流的角度去解析，复制之前的编码要和复制好后的编码要一致

为什么要进行URL编码
    URL编码只是简单的在特殊字符的各个字节前加上%，例如，我们对上述会产生奇异的字符进行URL编码后结果：“name1=va%26lu%3D”，
    这样服务端会把紧跟在“%”后的字节当成普通的字节，就是不会把它当成各个参数或键值对的分隔符。

    Url的编码格式采用的是ASCII码，而不是Unicode，这也就是说你不能在Url中包含任何非ASCII字符，例如中文。
    否则如果客户端浏览器和服务端浏览器支持的字符集不同的情况下，中文可能会造成问题。

    Url编码的原则就是使用安全的字符（没有特殊用途或者特殊意义的可打印字符）去表示那些不安全的字符；

    对于非ASCII字符，需要使用ASCII字符集的超集进行编码得到相应的字节，然后对每个字节执行百分号编码。对于Unicode字符，
    RFC文档建议使用utf-8对其进行编码得到相应的字节，然后对每个字节执行百分号编码。
    如"中文"使用UTF-8字符集得到的字节为0xE4 0xB8 0xAD 0xE6 0x96 0x87，经过Url编码之后得到"%E4%B8%AD%E6%96%87"。(使用百分号+两位十六进制字符来表示)

    对Unicode字符的编码方式不同：这三个函数对于ASCII字符的编码方式相同，均是使用百分号+两位十六进制字符来表示。
    但是对于Unicode字符，escape的编码方式是%uxxxx，其中的xxxx是用来表示unicode字符的4位十六进制字符。
    这种方式已经被W3C废弃了。但是在ECMA-262标准中仍然保留着escape的这种编码语法。encodeURI和encodeURIComponent则
    使用UTF-8对非ASCII字符进行编码，然后再进行百分号编码。这是RFC推荐的。因此建议尽可能的使用这两个函数替代escape进行编码。

    适用场合不同：encodeURI被用作对一个完整的URI进行编码，而encodeURIComponent被用作对URI的一个组件进行编码。从上面提到的安
    全字符范围表格来看，我们会发现，encodeURIComponent编码的字符范围要比encodeURI的大。我们上面提到过，保留字符一般是用来分
    隔URI组件（一个URI可以被切割成多个组件，参考预备知识一节）或者子组件（如URI中查询参数的分隔符），如：号用于分隔scheme和
    主机，?号用于分隔主机和路径。由于encodeURI操纵的对象是一个完整的的URI，
    这些字符在URI中本来就有特殊用途，因此这些保留字符不会被encodeURI编码，否则意义就变了。

    一般浏览器会自动进行url编码，form表单进行提交的时候也是自动url编码的（application/x-www-form-urlencoded）



    unicode编码是转换成该字符的unicode编号；


    乱码原因
    1、使用了不支持这种字符的字符集，会出现？？这样的形式，这种是不可逆的，因为编码的时候就已经将不支持的字符自动换成了？的编码，
        这就导致了在解码的时候就把文件中的二进制中的？号当成源文件，这样无论如何都不可能恢复了
    2缺少某种字体库，导致乱码，诸如“口”这样的乱码，这种只要下载相应的字体即可
    3、解码方式与编码方式不一致导致，这种只要统一编码方式即可

